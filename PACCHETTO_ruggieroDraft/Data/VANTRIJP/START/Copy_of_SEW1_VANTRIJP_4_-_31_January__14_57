Chopping Down the Syntax Tree 
(version 3)


Noam Chomsky revolutionized the field of linguistics in the 1950s, but is now facing to be dethroned himself. In the Chomskyan view, the human language faculty is an autonomous, genetically encoded “perfect system” that interfaces with other mental modules (Chomsky 1995). In the past few decades, however, a new approach has emerged that rejects this modular approach and that treats language as an integrated part of cognition and social interaction. This paradigm shift changes everything about how we think about our capacity to learn and use language.


The grammatical theory that embodies the counter-movement the most is called construction grammar. Construction grammars exist in different flavors, but they share the idea that all linguistic information can be represented using a single data structure: constructions. Unfortunately, construction grammar researchers have been reluctant to offer a precise definition of what a construction is. Instead, they informally describe their most important data structure as “any mapping between meaning and form”. This lack of formal rigour invites linguists to recycle the formal tools that were developed within mainstream linguistics, or even to abandon formalization completely. And without formally precise, testable analyses, Chomsky can keep his grip on the field.


Fortunately, recent developments in computational linguistics have provided construction grammar researchers with a methodological axe for finally chopping down the tree structures that occupy a central position in Chomskyan linguistics.


Construction grammarians feel that tree structures cannot adequately represent their linguistic analyses. Perhaps this is because most linguists are too young to remember the days before the Chomskyan revolution, so they have grown up knowing only the mathematically well-defined tree structures of mainstream linguistics. While there is some variation among theories, the core primitives of those tree structures are illustrated in Figure 1. First, all local tree configurations consist of a single parent node and its immediate children (e.g. Noun Phrase → Determiner Noun). Secondly, syntactic constituents such as the Noun Phrase must be continuous in the sense that they map onto an uninterrupted part of the sentence that is described by the tree. One useful by-product of these primitives is that you can describe the tree structure in a more condensed way. For instance, the structure of Figure 1 can be written using a bracketed notation as follows:


[ [ TheDETERMINER  stoneNoun  ]NOUN_PHRASE [ brokeVERB [ 




  

Figure 1: A Tree Diagram for the sentence “The stone broke the window.”




However, such tree diagrams are like the trees of royal gardens that have been clipped into perfect shapes or ornamental figures. Before Chomsky, linguists were very liberal with their analyses, allowing discontinuous constituents and nodes with multiple parents (McCawley 1982, Blevins 1990). McCawley (1982) writes that there has never been any real justification for introducing the current restrictions. He claims that they are the result of historical accident: early Chomskyan linguists were familiar with automata theory but not with graph theory, so they devised methods that were better suited for describing sets of strings than for sets of trees. Tree structures were therefore originally only the by-product for describing strings, so they had to be pruned to fit the surface form of those strings.


But even if you liberate syntactic trees from the constraints of single-dominance and continuity, you still end up with a single representation device from which you must “read off” all other kinds of information. For instance, grammatical functions such as Subject and Object have been reinterpreted as structural positions in mainstream linguistics: the subject is the node to the left of a sentence’s verb phrase, and the object is the node that is in the verb’s “immediate domain”. As such, grammatical functions are no longer primitive categories in mainstream linguistic theory. The same strategy applies for other layers of information such as argument structure and discourse structure. Construction grammarians no longer want to shoehorn those structures onto a syntax tree.


Functional theories of linguistics offer an alternative approach and propose a separate layer for dissimilar kinds of information. This solution, however, creates an interface problem because you need to define linking rules to connect each layer. Moreover, “all grammars leak” as Edward Sapir famously said, so it is not easy to neatly distinguish between different layers without introducing a great deal of redundancy. For example, a syntactic rule such as putting an adjective before a noun (as in the sleeping child or the scared man) may be sensitive to the phonological properties of the adjective, since the so-called a-adjectives resist occurring in this configuration (as in ??the asleep child or ??the afraid man). Likewise, morphological rules may depend on the semantics of a word: a mass noun such as water cannot be combined with the plural marker -s unless you conceptualize the water as an object (e.g. as two rivers, or as two glasses of water), in which case you can say two waters. Because of such problems, functional theories have never achieved the same degree of formalization as Chomskyan theories of language.


Enter Charles J. Fillmore, who established the criteria for what counts as a construction in his 1988 article “The Mechanisms of Construction Grammar”. First of all, a construction should be able to simultaneously access all kinds of linguistic information. If linguistic information is a multilayered cake, traditional linguists force you to eat the layer in a horizontal fashion: a scoop of phonology, a bite of morphosyntax, and a topping of semantics. Construction grammarians, however, can cut the cake vertically and gobble it up as a whole. Secondly, constructions are not restricted to local tree configurations as the theories of mainstream linguistics, nor are they bound to the constraints of tree representations. Instead they can directly access any information regardless of where that information is situated in a linguistic structure. In sum, constructions are data structures with unrestricted expressive power.


The increasing success of construction grammar in all branches of linguistics soon aroused the curiosity of formal and computational linguists, but it took until 2004 until the Belgian AI researcher Luc Steels and his teams at the SONY Computer Science Laboratories Paris and the VUB AI Research Lab in Brussels proposed a data structure that adheres to all of Fillmore’s criteria. Steels took the well-known practice of using feature structures for describing linguistic information, but added a little twist by organizing these feature structures into units. These units work as hyperlinks that give fast access to different parts of the feature structure. The unit structure allowed Steels to simultaneously represent different perspectives in the same data structure instead of taking one perspective as the primary representation device. For example, Steels (2004) included descriptions for both the dependency structure and the constituent structure of an utterance. 


Steels’ innovation has therefore solved both problems in linguistic representations. On the one hand, different kinds of linguistic information no longer have to be fitted onto a syntax tree. On the other hand, there is no need for separating the information in separate layers either. Constructions are in fact multidimensional data structures