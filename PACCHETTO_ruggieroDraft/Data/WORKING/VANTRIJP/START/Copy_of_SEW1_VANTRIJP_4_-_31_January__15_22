Chopping Down the Syntax Tree 
(version 3)


Noam Chomsky revolutionized the field of linguistics in the 1950s, but is now facing to be dethroned himself. In the Chomskyan view, the human language faculty is an autonomous, genetically encoded “perfect system” that interfaces with other mental modules (Chomsky 1995). In the past few decades, however, a new approach has emerged that rejects this modular approach and that treats language as an integrated part of cognition and social interaction. This paradigm shift changes everything about how we think about our capacity to learn and use language.


The grammatical theory that embodies the counter-movement the most is called construction grammar. Construction grammars exist in different flavors, but they share the idea that all linguistic information can be represented using a single data structure: constructions. Unfortunately, construction grammar researchers have been reluctant to offer a precise definition of what a construction is. Instead, they informally describe their most important data structure as “any mapping between meaning and form”. This lack of formal rigour invites linguists to recycle the formal tools that were developed within mainstream linguistics, or even to abandon formalization completely. And without formally precise, testable analyses, Chomsky can keep his grip on the field.


Fortunately, recent developments in computational linguistics have provided construction grammar researchers with a methodological axe for finally chopping down the tree structures that occupy a central position in Chomskyan linguistics.


Construction grammarians feel that tree structures cannot adequately represent their linguistic analyses. Perhaps this is because most linguists are too young to remember the days before the Chomskyan revolution, so they have grown up knowing only the mathematically well-defined tree structures of mainstream linguistics. While there is some variation among theories, the core primitives of those tree structures are illustrated in Figure 1. First, all local tree configurations consist of a single parent node and its immediate children (e.g. Noun Phrase → Determiner Noun). Secondly, syntactic constituents such as the Noun Phrase must be continuous in the sense that they map onto an uninterrupted part of the sentence that is described by the tree. One useful by-product of these primitives is that you can describe the tree structure in a more condensed way. For instance, the structure of Figure 1 can be written using a bracketed notation as follows:


[ [ TheDET  stoneNoun  ]NP [ brokeV [ theDET  windowN  ]NP  ]VP ]S




  

Figure 1: A Tree Diagram for the sentence “The stone broke the window.” Grammatical functions such as Subject and Object are defined as structural positions in the tree.




However, such tree diagrams are like the trees of royal gardens that have been clipped into perfect shapes or ornamental figures. Before Chomsky, linguists were very liberal with their analyses, allowing discontinuous constituents and nodes with multiple parents (McCawley 1982, Blevins 1990). McCawley (1982) writes that there has never been any real justification for introducing the current restrictions. He claims that they are the result of historical accident: early Chomskyan linguists were familiar with automata theory but not with graph theory, so they devised methods that were better suited for describing sets of strings than for sets of trees. Tree structures therefore only serve to generate strings, hence they have to be pruned to fit the linear order of these strings.


But even if you liberate syntactic trees from the constraints of single-dominance and continuity, you still end up with a single representation device from which you must “read off” all other kinds of information. For instance, grammatical functions such as Subject and Object have been reinterpreted as structural positions in mainstream linguistics, as shown in Figure 1: the subject is the node to the left of a sentence’s verb phrase, and the object is the node that is in the verb’s “immediate domain”. As such, grammatical functions are no longer primitive categories in mainstream linguistic theory. The same strategy applies for other layers of information such as argument structure and discourse structure. Construction grammarians no longer want to shoehorn those structures onto a syntax tree.


Functional theories of linguistics offer an alternative approach and propose a separate layer for dissimilar kinds of information. This solution, however, creates an interface problem because you need to define linking rules to connect each layer. Moreover, “all grammars leak” as Edward Sapir famously said, so it is not easy to neatly distinguish between different layers without introducing a great deal of redundancy. For example, the syntactic rule that states that English adjectives must precede a noun (Ex. 1-2) is sensitive to the phonological properties of the adjective. For instance, the class of “a-adjectives” (such as a-sleep and a-wake) resist obeying the syntax rule, which explains why ex. 3-4 sound awkward to English ears. Likewise, morphological rules often depend on the semantic properties of a word: a mass noun such as water cannot be combined with the plural marker -s unless you conceptualize the water as an object (e.g. as two rivers, or as two glasses of water), in which case you can say two waters. Because of such problems, functional theories have never achieved the same degree of formalization as Chomskyan theories of language.


(1)        the sleeping child                                (3)        ?? the asleep child
(2)        the scared man                                (4)        ?? the afraid man


Enter Charles J. Fillmore, who established the criteria for what counts as a construction in his 1988 article “The Mechanisms of Construction Grammar”. First of all, a construction should be able to simultaneously access all kinds of linguistic information. If linguistic information is a multilayered cake, traditional linguists force you to eat the layer in a horizontal fashion: a scoop of phonology, a bite of morphosyntax, and a topping of semantics. Construction grammarians, however, can cut the cake vertically and gobble it up as a whole. Secondly, constructions are not restricted to local tree configurations as the theories of mainstream linguistics, nor are they bound to the constraints of tree representations. Instead they can directly access any information regardless of where that information is situated in a linguistic structure. In sum, constructions are data structures with unrestricted expressive power.


The increasing success of construction grammar in all branches of linguistics soon aroused the curiosity of formal and computational linguists, but it took until 2004 until the Belgian AI researcher Luc Steels and his teams at the SONY Computer Science Laboratories Paris and the VUB AI Research Lab in Brussels proposed a data structure that adheres to all of Fillmore’s criteria. Steels took the well-known practice of using feature structures for describing linguistic information, but added a little twist by organizing these feature structures into units. These units work as hyperlinks that give fast access to different parts of the feature structure. These hyperlinks allowed Steels to simultaneously represent a sentence’s dependency structure and its constituent structure.


Steels’ innovation has solved the problems of both Chomskyan and functional theories of linguistics. On the one hand, a single data structure eliminates all the linking rules that you need for connecting the different layers of functional theories. On the other, different kinds of linguistic information no longer have to be fitted onto a syntax tree. This has major ramifications because the strong constraints of tree representations make it particularly hard for analyzing more complex utterances where the subject and object of a verb do not occur in their most common structural positions. This already happens as soon as you have a relative clauses such as the man you saw yesterday, in which the object of the verb (the man) precedes both the subject and the verb instead of following them. Mainstream linguists therefore have to introduce a batch of mechanisms on top of the core tree representation to accommodate for the data. In the Chomskyan “transformationalist” tradition, for instance, you need a “transformation” rule that modifies the tree by “extracting” the object from its original position