The human ability to anticipate a chunk of text when reading or listening at some phrases is a common tool used in daily life. Experiments in cognitive science shown that human readers actually read only a small part of the text chunks - mainly the beginning and some relevant parts as desinences and suffixes - reconstructing in autonomous and unconscious way the missing ignored parts. This mechainsm allows human readers to be unaware of eventual orthorgaphic mistakes because of changing attentional focus during the fruition of a textual document. A simple example is clearly represented by the previous sentence, where we treacherously inserted some errors and possibly not all the readers  have been aware of this anomaly.


Rubedo is a game explicitly conceived to exploit the above mentioned skill. It is an installation game aimed at the evaluation of the human ability to anticipate the next character of a written text. In this context, the player is asked to read the initial chunk of text of a short novel and to guess letter by letter the continuation of the story. This process is repeated up to the end of the novel and the final reward for the players is twofold: first they gain the comprehension and the full fruition of the reading, and besides they evaluate the guessing performance according to a reference score.


The main target of the present paper is to compare the efficiency in anticipate the future in textual context between humans and the AI technologies based on deep learning. Among the various aspects this research enlights, we have the opportunity to evaluate some of the strengths and weaknesses of the present AI systems. In particular, the comparison with the human behavior allowed to highlight some specific functional limits of the artificial neural systems processing temporal information.


Actually, on the human side, the ability to predict the next character is related to several cognitive aspects about the local and global comprehension of the narrative under reading. The local comprehension takes into account the knowledge of the words commonly used in a given language. This implies, for instance, that knowing the beginning of a word reduces the tree of the possible next characters, easily leading to the identification of the more likely subset of letters. At a more abstract level, a human reader identifies the most likely next text unit relying on a full comprehension of the overall narrative. This aspect is strictly related to the general meaning of the script, which allows the selection of the best context from which to extract the most appropriate word.


In 1951, Claude Shannon investigated the aforementioned ability to anticipate the next character as a paradigm of an efficient compressor of information, building at the same time the foundations of an algorithmic process. The experiment proposed by Shannon consists of two twins at both the ends of a transmission channel. Given a text to be sent, the sender twin A performs the same anticipation process as described above, attempting to predict the next character from the very beginning and annotating how many attempts have been required for every letter in the message. Therefore, the resulting numerical sequence describes the amount of attempts per letter, where the best cases are represented by 1 - the sender guessed the right letter at the first attempt - and the cases labelled with a number greater than 1 represent the multi-attempt case. Basing on this, Shannon conceived the specific compression technique: the sender either transfers the characters labelled with a number greater than one, or a blank jolly character for the correctly guessed letters. The other twin B receives such a coded message and, assuming that his mental processes are perfectly identical to A, the blank character can be guessed  with probability one. The conception of this compression system led Shannon to the definition of a upper and a lower bound for the information entropy related to the language used in the game. He proved that entropy estimation bounds are only related to the distribution of the number of attempts performed by the human player.


In light of the above, Rubedo is more than a simple installation to make people playing. It is a tool aimed at estimating the entropy of a temporal information signal such as textual documents. This tool allows to replicate the experiment conducted by Shannon, where he shown the entropy bounds trend as the length of the text chunk increases, providing an estimation of the long term entropy upper bound of English language of about 1 bit per character.


However, the inclusion of the AI player makes Rubedo innovative with respect to the Shannon work. It not only allows to gather data about human performance in anticipating letters, and consequently confirming the results obtained by Shannon. Rubedo pushes the research to both improve the artificial generative systems to be more and more efficient to convincingly emulate the human behavior, and to consider the emerging limitations of the artificial systems as a productive starting point to elaborate new methods and architectures.