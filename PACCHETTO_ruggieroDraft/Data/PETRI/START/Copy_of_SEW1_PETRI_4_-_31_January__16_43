How we learn the shape of spaces and their shapes affect our thinking


AI started a new gold rush. It is causing huge changes in industry, academia and in society. Companies rush to hire young deep-learning wizards. Universities restructure their courses to include all types of machine learning. Traditionally boring computer science conferences revamp themselves into dazzling shows of socially disrupting applications and egos. It wasn’t always the case. This AI renaissance follows decades of slow development and rides on the wave of the explosion of computing power and availability of large annotated datasets. The combination of these two allowed neural networks to outperform other competing technologies at last. Deep networks now reach human-level performances in complex tasks, such as driving a car, planning a path through a crowded station and -importantly- recognize fluffy cats in photographs. 


However, AI was about something else originally. It was about understanding the origin of human intelligence. And whether we could we reconstruct it. This question inspired Alan Turing in 1948 to theorize the existence of unorganized machines. These were machines which could gradually learn and eventually resemble human intelligence. Unorganized machines in turn inspired the first models of neural networks. These were crude attempts at modeling the apparently incoherent hairball of neurons inside our head and promised to open a window into what made us able to learn, think, plan, live. These initial promise however was by and large broken during the AI winter of the 70s and 80s. 


At long last, AI systems shine some light of how the human brain works. In fact, recent research  in deep networks with human-level performances showed that they can be used to explicitly compare compare how humans and machines learn and perform tasks. And in many cases, deep networks are more than simple toy models. They appear to be doing the very same thing. As an example, the deep activations of convolutional networks trained to recognize images mimic those of the human visual areas. When trained on sensory inputs instead, deep networks approximate the human sensory cortical processing. 


This similarity between deep networks and brains goes deeper. Neural networks and the human brain appear to represent space in the same way. Recent research showed that neural networks trained to perform spatial navigation develop units that resemble the behaviour of neurons responsible for encoding space. These cells fall in two categories. The first type, called place cells, fire when the animal is near a specific position in space. Populations of such cells thus create an atlas, similar to a patchwork, that covers the space the animal inhabits. The second type, grid cells, is even more striking: each one fires when the animal crosses the corners of a hexagonal grid that covers space. Jointly they provide the animal with a navigation grid. 


These shared navigational structures are topological in nature. Topology is a quite arcane branch of mathematics. It describes the connectivity and the shape of arbitrary spaces. Scholars usually consider it beautiful, and of little practical use. For example, in 1968 a character of the Russian writer E. Solzhenitsyn could be heard exclaiming: “Topology! The stratosphere of human thought! In the twenty-fourth century it might possibly be of use to someone...”. It turns that was a pessimistic prediction. Recent research showed that populations of neurons trying to encode space in a parsimonious way develop grid cells. While the shape of the lattice of such grid cells depends on the environment dimension (2D for mice, 3D for flying bats), it always matches that obtained with the densest packing of spheres in the corresponding dimension. Densest packings however are a purely topological concept because they only rely on the connectivity pattern of the spheres, rather than on their size. 


This link between topology and optimal compression vanishes in higher dimensions. In fact, the correspondence between optimal encoding and topology of spaces is rooted in the fact that in low dimensional spaces connectivity defines geometry. Formally, the topological structure defines the differential structure. This however is no longer true for dimensions higher than 3. For most of dimensions larger than 4 there exist multiple differential structures compatible with the same topology. It is unclear then what would then happen to our capacity to explore and navigate such spaces, despite the practical evidence that the same structures involved in spatial navigation are involved in navigation of richer spaces, such as memory or social spaces. 


Here we show that the grid structure learned by deep networks navigating in high dimensions correspond/does not correspond to the simplest topological packing.  While it is hard to study higher dimensional exploration in live animals, it is rather straightforward in the deep networks. We thus  train networks to integrate paths of synthetic mice living in progressively higher dimensions and show how the response patterns of grid-like units change with dimensionality. We characterize their topological structure using recent topological data analysis tools and relate them to the prediction of optimal compression theory. 
(if it works) Finally, we prove bounds on the entropic of the learned lattice and estimate the dimensionality of memory spaces using data from memory-recollection experiments. 
(it it doesn’t) Finally, we relate the novel grid cell structures detected to the patterns observed in data from memory-recollection experiments.