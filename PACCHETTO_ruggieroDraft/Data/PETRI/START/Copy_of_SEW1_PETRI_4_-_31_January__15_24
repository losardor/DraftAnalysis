AI started a new gold rush. It is causing huge changes in industry, academia and in society. Companies rush to hire young deep-learning wizards. Universities restructure their courses to include all types of machine learning. Traditionally boring computer science conferences revamp themselves into dazzling shows of socially disrupting applications and egos. 


It wasn’t always the case. This AI renaissance follows decades of slow development and rides on the wave of the explosion of computing power and availability of large annotated datasets. The combination of these two allowed neural networks to outperform other competing technologies at last. Deep networks now reach human-level performances in complex tasks, such as driving a car, planning a path through a crowded station and -importantly- recognize fluffy cats in photographs. 


However, AI was about something else originally. It was about understanding the origin of human intelligence. And whether we could we reconstruct it. This question inspired Alan Turing in 1948 to theorize the existence of unorganized machines. These were machines which could gradually learn and eventually resemble human intelligence. Unorganized machines in turn inspired the first models of neural networks. These were crude attempts at modeling the apparently incoherent hairball of neurons inside our head and promised to open a window into what made us able to learn, think, plan, live. These initial promise however went by and large unkept during the AI winter of the 70s and 80s. 


At long last, AI systems shine some light of how the human brain works. In fact, recent research  in deep networks with human-level performances showed that they can be used to explicitly compare compare how humans and machines learn and perform tasks. And in many cases, deep networks are more than simple toy models. They appear to be doing the very same thing. As an example, the deep activations of convolutional networks trained to recognize images mimic those of the human visual areas. When trained on sensory inputs instead, deep networks approximate the human sensory cortical processing. 


This similarity between deep networks and brains goes deeper. Neural networks and the human brain appear to represent space in the same way. Indeed, neural networks trained to perform spatial navigation display units that resemble the behaviour of neurons responsible of encoding space. These cells line in the mammalian hippocampus and fall in two categories. The first type, called place cells, fire when the animal is near a specific position in space. Populations of such cells thus create an atlas, similar to a patchwork, that covers the space the animal inhabits. The second type, grid cells, is even more striking: each one fires when the animal crosses the corners of a hexagonal grid that covers space. Jointly they provide the animal with a navigation grid. 


These shared navigational structures are topological in nature. Topology is a quite arcane branch of mathematics. It describes the connectivity and the shape of arbitrary spaces. Scholars usually consider it beautiful, and of little practical use. For example, in 1968 a character of the Russian writer E. Solzhenitsyn could be heard exclaiming: “Topology! The stratosphere of human thought! In the twenty-fourth century it might possibly be of use to someone...”. It turns that was a pessimistic prediction. Recent research showed that populations of neurons trying to encode space in a parsimonious way develop grid cells. While the shape of the lattice of such grid cells depends on the environment dimension (2D for mice, 3D for flying bats), it always matches that obtained with the densest packing of spheres in the corresponding dimension. Densest packings however are a purely topological concept. This in turn suggests that the topological representation of spaces relies on the equivalence of topological and differential structures in low dimensional spaces. 


The link between topological and differential structures vanishes in higher dimensions. In fact, the correspondence between stochastic encoding and topology of spaces is rooted in the fact that in low dimensional spaces the topological structure defines the differential structure. This however is no longer true for dimensions higher than 3. This means that the topology of the space does not determine the geometry of it. It is unclear then how we, and deep learning systems, are able to produce a navigational map to navigate the more complex spaces that lie behind complex cognitive tasks, such as memory spaces or value spaces. 


Final paragraph 1 - Article intro: 
Here we show that the grid structure learned by deep networks navigating in high dimensions correspond/does not correspond to the simplest topological packing.  dfnsofanofan




Final paragraph 2 - commentary/story: 






Notes:
- I dont know whether to keep the second paragraph
- I dont know where to put the billboard paragraph
- paragraphs are not balanced and well-divided anymore…